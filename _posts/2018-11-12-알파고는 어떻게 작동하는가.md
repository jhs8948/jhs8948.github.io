---
layout: post
title:  "알파고는 어떻게 작동하는가?"
date:   2018-11-12 20:02:13
categories: Data_science
permalink: /archivers/python_lecture_03
---

##2018-11-12-알파고는 어떻게 작동하는가?.md

#__알파고는 어떻게 작동하는가?__

기본 동작 원리바둑은 수가 너무 많기 때문에 brute force 방식의 '일단 다 따져보자!'는 사실상 불가능합니다. 그렇다고 임의로 찍자니 이게 제대로 된 수인지 객관적으로 판별할 방법이 없죠. 알파고는 이 문제를 해결하기 위해 1) 경우의 수를 줄이면서 2) 결과를 더 빨리 예측하는데에 집중합니다.

__1) 경우의 수 줄이기(Breadth Reduction)__

![alpha1](https://i.imgur.com/O15qXmV.png)

위와 같은 수가 놓였다고 할 때, brute force 방식으로는 다음에 놓일 수 있는 모든 수를 다 따져봐야 합니다. 이건 가장 확실하지만 동시에 가장 비효율적이기도 하죠. 알파고는 경우의 수를 줄이기 위해 학습한 기보를 기반으로 상대가 이 다음에 놓지 않을 수를 제거합니다. 기보를 학습해보니 이 다음엔 여기에 놓는 경우가 없더라..하는 식으로요.

![alpha2](https://i.imgur.com/AgbrAFz.png)

이렇게 하면 경우의 수를 획기적으로 줄일 수 있습니다. 제가 바알못이라 바둑을 정확하게 알지는 못 하지만, 사람간의 모든 턴제 대전이 그렇듯 '말도 안 되는 수'와 '합리적인 수'는 구분되기 마련이고 알파고는 여기에 착안하여 이 다음에 올 수 없는 수를 제거하여 가능한 경우의 수를 줄입니다.

__2) 결과 더 빨리 예측하기(Depth Reduction)__
경우의 수를 줄인 건 좋은데, 여기에도 맹점은 있습니다. 이후에 이어질 수를 예측하고 이게 대국의 끝까지 진행되었을 때 내가 이기냐 지냐를 판별해야 하는데, 한 수 둘 때마다 이러면 brute force에 비해 줄었다고는 하나 여전히 시간이 엄청 오래 걸리게 됩니다. 알파고는 이를 해결하기 위해 depth reduction이라는 프로세스를 수행하는데, 이는 대국이 진행되면서 내가 다음에 두는 수의 가치는 어느 정도인가?를 수치로 나타내어 대국의 끝까지 예측하지 않고 이 정도면 됐어!하는 지점에서 멈춰 예측을 보다 빨리 하는 과정입니다.

![alpha3](https://i.imgur.com/STuXcXz.png)


위 그림에서 d를 대국의 진행 정도, 각각의 예측을 s, V를 수의 가치라고 하면, V(s)를 s라는 수를 두었을 때의 유/불리로 볼 수 있습니다. 이를 통해 대국의 끝까지 예측하지 않아도 현재 대국의 판세를 판단하고 다음에 둘 수를 보다 빨리 결정할 수 있게 됩니다.


__학습은 어떻게 했는가?__
4주간 3천만개의 착점으로 구성된 16만개의 고수들의 기보를 학습하고 이를 기반으로 자가 대국 100만 판을 해서, 인간이 하루에 3번의 대국을 한다고 쳤을 때 약 1천년치에 해당하는 훈련을 거쳤다고 합니다. 이 과정을 좀더 자세하게 풀면..

__1) Supervised Learning'__
16만개의 기보'를 학습하는 과정은, 현재의 수 및 반상의 판세를 바탕으로 '지금은 이랬는데 다음엔 저렇게 두는게 많더라'하는 식으로 이루어집니다. 다시 말해, 프로 기사 등 전문가들의 기보를 학습하면서 각 수와 수 사이에 예측 모델을 두고 이 예측 모델에 다음에 행해지는 수를 다 입력하는 방식입니다.

예를 들어, 아래와 같이 대국이 진행되는 기보가 있을 때..

![alpha4](https://i.imgur.com/Dbaueel.png)
이 기보에서는 이 사람이 이렇게 두었더라..라고 학습합니다. 이런 학습 결과를 모으고 모아서,

![alpha5](https://i.imgur.com/VtCIncg.png)

L:p(next action | current state)로 표현할 때, 이런 식으로 다음 착점의 가치를 계산하고 가장 승률이 좋았던 곳은 여기구나! 하는 식으로 학습합니다. 이런 식으로 판세를 읽는 데에는 입력값을 레이어별로 추상화하여 출력값을 뽑아내는데 매우 유용한 Convolutional Neural Network(CNN)이라는 기법이 사용됐습니다. 그냥 그런게 쓰였구나..하고 넘어가셔도 됩니다..^^;

__2) Reinforcement Learning__
알파고의 학습에서 핵심이 되는 파트입니다. 쉽게 말해, 입력된 기보를 바탕으로 자가 대국을 수행하여 상황에 따른 다음 수별 승률을 계산하고 최적의 수를 찾는 학습 과정입니다.

![alpha6](https://i.imgur.com/nzlo3AP.png)
![alpha7](https://i.imgur.com/49kKhD8.png)
예를 들어, 같은 상황에서 예측 모델을 거쳐 다음 수를 이렇게 두니까 졌는데 저렇게 두니까 이기더라.. 하는 식으로 자가 대국을 거쳐 수의 가치를 보정하고 이를 통해 반복적으로 자가 업데이트를 수행하여 끊임없이 발전해나갑니다. 알파고 논문에서는 학습 완료 후 모델과 최초의 모델을 대국시켜봤을 때 학습이 완료된 모델이 80% 승률을 기록했다고 합니다.

요약 정리이상을 요약하면, 알파고는
![alpha8](https://i.imgur.com/LikipDF.jpg)
전문가의 기보를 바탕으로 학습을 하고 자가 대국을 통해 발전하며, 실전 대국시에는 1) 경우의 수를 줄이고(Breadth Reduction) 2) 판세를 판단하고 결과를 더 빨리 예측(Depth Reduction)하는 인공지능입니다. 대국 메커니즘을 정리하면 아래와 같으며, 이런 방식을 몬테 카를로 탐색 트리 기법이라고 합니다.
![alpha9](https://i.imgur.com/XAepkoW.jpg)
a. Selection이게 괜찮은 수인지, 되도 않는 수인지를 판단하여 경우의 수를 줄입니다.(Policy Network)
b. Expansion,
c. Evaluation보다 빠른 예측을 거쳐 판세를 판단하고 수의 가치를 평가합니다.(Rollout, Value Network) 논문에서는 이 과정을 거쳐 2us안에 다음 수를 결정할 수 있다고 설명하고 있습니다.
d. Backupb와 c과정의 결과값을 합쳐 최종적으로 수를 예측하고 결정합니다.